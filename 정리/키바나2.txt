분석기
- 전문 검색을 지원하기 위해 역인덱싱 기술을 사용한다.

역인덱싱
- 장문의 문자열을 분석해 작은 단위로 쪼개(토큰)는 검색하는 기술!

분석기 구성
- 캐릭터 필터 : 입력받은 문자열을 변경하거나 불필요한 문자열을 제거한다.(전처리 작업)
- 토크나이저 : 문자열을 토큰으로 분리한다.
                    분리할 때 토큰의 순서나 시작, 끝 위치도 기록한다.
- 토큰 필터 : 분리된 토큰들의 필터 작업한다.
                  대소문자 구분, 형태소분석 등 작업을 한다.
                  전문 검색에 활용하게 된다.

분석기 API
- 공백을 기준으로 문자를 구분하는 것!
- 별도 설정이 없을 경우 엘라스틱 서치가 기본적으로 사용하는 분석기로 공백이나 특수문자('.'제외)를 경계로 문장을 토큰화한다.
- 영문법을 기준으로 한 standard 토크나이저와 소문자 변경 필터가 포함되어 있어 모든 대문자를 소문자로 변경한다.
POST _analyze
{
   "analyzer" : "standard",
   "text" : [
       "텍스트"
    ]
}

POST _analyze
{
  "analyzer": "standard",
  "text": [
    "The 10 most \nLoving \ndog Breeds."
  ]
}

분석기 사용법
- "_analyzer" 지정하고
- "text"에 분석할 문자열을 입력하면 된다.
- 기본적으로 제공하는 분석기 : standard, whitespace, stop, simple 등

whitespace : 공백을 경계로 문장을 토큰화하고 whitespace(\n)은 토큰화 하지 않는다.
POST _analyze
{
  "analyzer": "whitespace",
  "text": [
    "The 10 most \nloving \ndog breeds."
  ]
}


simple 분석기
- standard 분석기 기능을 포함하고있고 숫자는 토큰화하지 않는다.
- .을 구분자로 사용한다
POST _analyze
{
  "analyzer": "simple",
  "text": [
    "The 10 most \nLoving \ndog Breeds."
  ]
}


stop 분석
- simple분석기 기능을 포함하고 있고 stop 필터가 포함되어 있어서 the 불용어가 제거된다.

- 불용어 : 데이터 빈도수는 매우 높지만 의미가 없는 단어 제거
POST _analyze
{
  "analyzer": "stop",
  "text": [
    "The 10 most \nLoving \ndog Breeds."
  ]
}


토크나이저(tokenizer)
- 토크나이저는 문자열을 분리해 토큰화하는 역할한다.
- 분석기에 반드시 포함되어야하기 때문에 형태에 맞는 토크나이저 선택이 중요하다.
POST _analyze
{
  "tokenizer": "사용할 토크나이저",
  "text": [
    "텍스트"
  ]
}

standard 토크나이저
- 엘라스틱 서치에서 기본적으로 제공하는 토크나이저
POST _analyze
{
  "tokenizer": "standard",
  "text": [
    "email: Elastic@lk-Company.com"
  ]
}


lowercase 토크나이저
- 토큰화한 결과를 모두 소문자로 변경하고 공백과 '.'을 포함한 모든 특수문자가 구분자로 사용됨


uax_url_email 토크나이저
- url이나 이메일을 토큰화한다.
POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": [
    "email: Elastic@lk-Company.com, url: https://www.naver.com"
  ]
}


@필터(filter)
 분석기는 하나의 토크나이저와 다수의 필터로
 조합을 할 수있다.
 필터가 없는 분석기는 토크나이저만 이용해서 
 토큰화 작업을 진행하는데 엘라스틱서치에서 
 제공하는 분석기들은 하나 이상의 필터를 
 포함하고 있고 필터를 통해서 더 세부적인
 작업이 가능하다. 
 필터는 단독으로 사용할 수없고 반드시 
 토크나이저와 같이 사용해야된다.


# POST _analyze
# {
#   "tokenizer": "사용할 토크나이저",
#   "filter": [
#     "사용할 필터", ...
#   ],
#   "text": [
#     "텍스트"
#   ]
# }

@Lowercase ,Uppercase 필터
 대문자를 모두 소문자로 변경하는 필터
 대소문자를 구분하지 않는 검색을 가능하
 기 위해서!

POST _analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "lowercase"
  ],
  "text": [
    "Elasticsearch IS Awesome!"
  ]
}

@Lowercase + Stop filter 적용
 불용어 ( 검색에 중요하지 않는 단어 )제거하
            는 필터 ( the, is)

POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase",
    "stop"
  ],
  "text": [
    "The quick brown fox jumps over the lazy dog."
  ]
}

@ Edge N-gram 필터 
  단어의 시작 부분 일부만을 토큰으로 분리하  
  여 검색에 사용한다.
 
  "min_gram" 최소 몇 글자부터 분리
  "max_gram" 최대 몇 글자까지 분리
POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase",
    {
      "type": "edge_ngram",
      "min_gram": 1,
      "max_gram": 3
    }
  ],
  "text": [
    "Elasticsearch"
  ]
}
@ 위에 기능은 자동완성 기능을 사용할 때 
  일부만 입력을 해도 같은 단어를 미리 검색
  해서 처리할 수있도록!

@ stop 필터 
 엘라스틱서치에 제공하는 분석기 중 원하는
 기능을 만족하는 분석기가 없을 때 사용자가
 직접 토크나이저와 필터를 조합해서 
 만들어 사용하는 커스텀 분석기에 
 기본 필터에서 제거하지 못하는
 특정 단어를 제거할 수있다. 
 영어를 기반으로 하기 때문에 한글에서는
 잘 동작하지 않을 수있다.

@Stemmer 필터
 단어에서 불필요한 글자를 때에내어 
 기본적인 형태로 변환하는 필터!

 running , ran, runs --> run

POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase",
    "stemmer"
  ],
  "text": [
    "running ran runs"
  ]
}

POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase",
    "stemmer"
  ],
  "text": [
    "jumps jumping jumped"
  ]
}

POST _analyze
{
  "tokenizer": "standard",
  "filter": [
    "lowercase",
    "stemmer"
  ],
  "text": [
    "cars buses foxes"
  ]
}

@커스텀 분석기를 적용한 인덱스 생성
# PUT customer_analyzer
# {
#   "settings": {
#     "analysis": {
#       "filter": {
#         "커스텀 필터 이름": {
#           "type": "사용할 필터",
#           필터 설정
#         }
#       },
#       "analyzer": {
#         "커스텀 분석기 이름": {
#           "type": "custom",
#           "char_filter": ["사용할 캐릭터 필터", ...],
#           "tokenizer": "사용할 토크나이저",
#           "filter": ["사용할 토큰 필터", ...]
#         }
#       }
#     }
#   }
# }

@
PUT customer_analyzer
 {
   "settings": {
     "analysis": {
       "filter": {
         "my_stopwords": {
          "type": "stop",
          "stopwords":["lions","the"]
         }
       },
       "analyzer": {
         "my_analyzer": {
           "type": "custom",
           "char_filter": [],
           "tokenizer": "standard",
           "filter": ["my_stopwords","lowercase"]
         }
       }
     }
   }
 }

"settings"  블록을 지정해서 필터,토크나이저,분석기 설정
"analysis"  분석기,필터를 설정
"filter" 
"stopwords" [] 지정된 단어를 제거!
"char_filter" 텍스트에서 특정 문자나 패턴을 수정하거나 삭제하는
                      역할이다. 빈리스트로 설정을 하면 문자 필터를 
                      사용하지 않겠다

GET customer_analyzer/_mapping

GET customer_analyzer/_analyze
{
    "analyzer": "내가만든 분석기 이름",
       "text" : [
	문자
    ]
}


GET customer_analyzer/_analyze
{
    "analyzer": "my_analyzer",
       "text" : [
	"The Cats Lions Dogs"
    ]
}

@ 필터를 여러개 사용하면 적용할 필터 순서에도 주의
["my_stopwords","lowercase"] "my_stopwords"가 먼저 실행
되고 난 후  "lowercase"가 실행되서 불용어를 먼저 제거한 후 
소문자로 변경하는데 stopwords 대소문자 구별해서 제거를 하기 때문에 대소문자를 먼저 변경하고 lowercase,uppercase 한 다음에
제거를 해야된다.

@ 기존에 내가 만든 분석기 제거
DELETE customer_analyzer

PUT customer_analyzer
 {
   "settings": {
     "analysis": {
       "filter": {
         "my_stopwords": {
          "type": "stop",
          "stopwords":["lions","the"]
         }
       },
       "analyzer": {
         "my_analyzer": {
           "type": "custom",
           "char_filter": [],
           "tokenizer": "standard",
           "filter": ["lowercase","my_stopwords"]
         }
       }
     }
   }
 }
@ 조회시
GET customer_analyzer/_analyze
{
    "analyzer": "my_analyzer",
       "text" : [
	"The Cats Lions Dogs"
    ]
} 
@결과
{
  "tokens" : [
    {
      "token" : "cats"     
    },
    {
      "token" : "dogs"      
    }
  ]
}

PUT customer_analyzer
 {
   "settings": {
     "analysis": {
       "filter": {
         "my_stopwords": {
          "type": "stop",
          "stopwords":["lions","the"]
         }
       },
       "analyzer": {
         "my_analyzer": {
           "type": "custom",
           "char_filter": [],
           "tokenizer": "standard",
           "filter": ["lowercase","my_stopwords"]
         }
       }
     }
   }
 }